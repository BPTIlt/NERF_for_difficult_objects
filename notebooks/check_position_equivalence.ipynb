{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "ROOT_PATH = Path(\"../../data/llff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 17)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load reference poses\n",
    "REFERENCE_PATH = ROOT_PATH / \"deer_v6_debug_script_picked_blurry\"\n",
    "reference_poses = np.load(str(REFERENCE_PATH / \"poses_bounds.npy\"))\n",
    "reference_poses.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(202, 17)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load poses that we expect to be a linear transformation of the reference poses\n",
    "DESIRED_RESULT_PATH = ROOT_PATH / \"deer_v6_debug\"\n",
    "desired_result_poses = np.load(str(DESIRED_RESULT_PATH / \"poses_bounds.npy\"))\n",
    "desired_result_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "114"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: sort\n",
    "\n",
    "# Match desired_result_poses to to reference poses by image that each corresponds to\n",
    "reference_images = sorted([f for f in (REFERENCE_PATH / \"images\").iterdir()])\n",
    "desired_result_images = sorted([f for f in (DESIRED_RESULT_PATH / \"images\").iterdir()])\n",
    "assert len(reference_images) == reference_poses.shape[0]\n",
    "assert len(desired_result_images) == desired_result_poses.shape[0]\n",
    "\n",
    "indices_of_matched_images_from_desired = list(filter(\n",
    "    lambda x: str(x[1].name) in list(map(lambda y: str(y.name), reference_images)),\n",
    "    enumerate(desired_result_images)\n",
    ")) # List comprehension\n",
    "assert len(indices_of_matched_images_from_desired) == len(reference_images)\n",
    "\n",
    "indices = list(map(lambda x: x[0], indices_of_matched_images_from_desired))\n",
    "len(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 17)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "desired_result_poses = desired_result_poses[indices, :]\n",
    "desired_result_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114, 15), (114, 15))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop near/far bounds\n",
    "if reference_poses.shape[-1] == 17:\n",
    "    reference_poses = reference_poses[:, :-2]\n",
    "if desired_result_poses.shape[-1] == 17:\n",
    "    desired_result_poses = desired_result_poses[:, :-2]\n",
    "\n",
    "reference_poses.shape, desired_result_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26777061176434863"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normalize\n",
    "X = StandardScaler().fit_transform(reference_poses)\n",
    "y = StandardScaler().fit_transform(desired_result_poses)\n",
    "\n",
    "# Fit linear mdel\n",
    "linear_model = LinearRegression().fit(X, y) # Use default values\n",
    "predictions = linear_model.predict(X)\n",
    "\n",
    "# Use squared error\n",
    "mean_squared_error(X, y, squared=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[8.12824882e-04, 1.32087882e-03, 8.72954584e-04, 8.73939239e-04,\n",
       "        0.00000000e+00],\n",
       "       [1.56504821e-03, 8.10295527e-04, 1.19404283e-03, 1.20442626e-03,\n",
       "        0.00000000e+00],\n",
       "       [1.63307841e-03, 5.70530234e-04, 2.54969905e-03, 3.15145841e-03,\n",
       "        4.00000000e+00]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mean_squared_error(X, y, squared=True, multioutput=\"raw_values\") # good\n",
    "loss.reshape((3, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0013799313721024158, 0.037147427530078256)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop img wh, focal length and normalize\n",
    "X = StandardScaler().fit_transform(\n",
    "    reference_poses.reshape((reference_poses.shape[0], 3, 5))[:, :, :-1].reshape((reference_poses.shape[0], -1))\n",
    ")\n",
    "y = StandardScaler().fit_transform(\n",
    "    desired_result_poses.reshape((desired_result_poses.shape[0], 3, 5))[:, :, :-1].reshape((desired_result_poses.shape[0], -1))\n",
    ")\n",
    "\n",
    "# Fit linear mdel\n",
    "linear_model = LinearRegression().fit(X, y) # Use default values\n",
    "predictions = linear_model.predict(X)\n",
    "\n",
    "# Use squared error\n",
    "error = mean_squared_error(X, y, squared=True)\n",
    "error, np.sqrt(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00081282, 0.00132088, 0.00087295, 0.00087394],\n",
       "       [0.00156505, 0.0008103 , 0.00119404, 0.00120443],\n",
       "       [0.00163308, 0.00057053, 0.0025497 , 0.00315146]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = mean_squared_error(X, y, squared=True, multioutput=\"raw_values\") # good\n",
    "loss.reshape((3, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e79e6f44a054bb329d91965122957da60628be052773fa250940d0208d63cbd"
  },
  "kernelspec": {
   "display_name": "Python 3.6.13 64-bit ('nerf_pl': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
